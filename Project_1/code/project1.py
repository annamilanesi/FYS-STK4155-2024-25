# -*- coding: utf-8 -*-
"""Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17tuzBfiMbLmPnD7LrLFfQqjicC0bS6sF

# Code
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
import numpy as np
from random import random, seed
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.utils import resample
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import seaborn as sns  # import the seaborn library
from matplotlib.colors import LogNorm
from sklearn.model_selection import cross_val_score
from imageio import imread

"""# Def"""

def FrankeFunction(x,y):
    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))
    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))
    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))
    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)
    return term1 + term2 + term3 + term4

#calculate MSE
def calculateMSE(f, fpred):
  mse=np.mean((f-fpred)**2)
  return mse

#calculate R2
def R2(f_data, f_model): #calculate R
    return 1 - (np.sum((f_data - f_model) ** 2) / np.sum((f_data - np.mean(f_data)) ** 2))


#def function to calculate the design matrix
def design_matrix(x, y, degree):
    # x and y are expected to be 1-dimensional arrays
    x = x.ravel()  # Flatten x
    y = y.ravel()  # Flatten y
    N = len(x)
    l = int((degree + 1) * (degree + 2) / 2)  # Number of polynomial terms
    X = np.ones((N, l))

    idx = 0
    for i in range(degree + 1):
        for j in range(i + 1):
            X[:, idx] = (x ** (i - j)) * (y ** j)
            idx += 1
    return X

# beta OLS
def calcuateB(X, f):
  beta = np.linalg.pinv(X.T @ X) @ X.T @ f
  return beta


# beta Ridge
def betaRIDGE(X, f, lm):
  I = np.identity(X.shape[1])  # Identity matrix
  beta_RIDGE = np.linalg.inv(X.T @ X + lm * I) @ X.T @ f
  return beta_RIDGE

#function to split the data into train and test and scale them
def split_and_scale(X, f, test_size=0.2, with_std=True):
    X_train, X_test, f_train, f_test = train_test_split(X, f, test_size=test_size)
    scaler_X = StandardScaler(with_std=with_std)
    scaler_X.fit(X_train)
    scaler_f = StandardScaler(with_std=with_std)
    scaler_f.fit(f_train.reshape(-1, 1))

    X_train_scaled = scaler_X.transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    f_train_scaled = scaler_f.transform(f_train.reshape(-1, 1)).ravel()
    f_test_scaled = scaler_f.transform(f_test.reshape(-1, 1)).ravel()

    return X_train_scaled, X_test_scaled, f_train_scaled, f_test_scaled

"""# 400 point

"""

np.random.seed(7935)

fig = plt.figure()
#ax = fig.gca(projection='3d')
ax = fig.add_subplot(projection='3d')
# Make data.
x = np.arange(0, 1, 0.05)
y = np.arange(0, 1, 0.05)
x, y = np.meshgrid(x,y)
# 400 data

z = FrankeFunction(x, y)

# Plot the surface.
surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm, linewidth=0, antialiased=False)

# Customize the z axis.
ax.set_zlim(-0.10, 1.40)
ax.zaxis.set_major_locator(LinearLocator(10))
ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()

#add the noise to the franke function
f = FrankeFunction(x,y) + np.random.normal(0, 0.1, z.shape)

fig = plt.figure()
#ax = fig.gca(projection='3d')
ax = fig.add_subplot(projection='3d')

# Plot the surface of the function with the added noise
surf = ax.plot_surface(x, y, f, cmap=cm.coolwarm, linewidth=0, antialiased=False)

# Customize the z axis.
ax.set_zlim(-0.10, 1.40)
ax.zaxis.set_major_locator(LinearLocator(10))
ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()

#flatten the franke function value into an array (it was a grid since we used mashgrid)
f = f.ravel()

"""# OLS"""

#create vectors to store data
np.random.seed(7935)
maxdegree = 20
polydegree = np.zeros(maxdegree)
MSE_train = np.zeros(maxdegree)
MSE_test = np.zeros(maxdegree)
R2_train = np.zeros(maxdegree)
R2_test = np.zeros(maxdegree)
betas = []

# for loop for every polynomial degree
for degree in range (maxdegree):

    # create design matrix
    X = design_matrix(x, y, degree)
    #divide into train and test data
    X_train, X_test, f_train, f_test = train_test_split(X, f, test_size=0.2)
    # calculate beta
    beta = calcuateB(X_train, f_train)
    betas.append(beta)
    #make the predictions
    ftilde = X_train @ beta
    fpred = X_test @ beta
    #fill the vectors with the data
    polydegree[degree] = degree
    MSE_train[degree] = calculateMSE(f_train, ftilde)
    MSE_test[degree] = calculateMSE(f_test, fpred)
    R2_train[degree] = R2(f_train, ftilde)
    R2_test[degree] = R2(f_test, fpred)

    #print the results
    print(f"Degree: {degree}")
    #print(f"Beta: {beta}")
    print(f"MSE Train Data: {MSE_train[degree]}")
    print(f"MSE Test Data: {MSE_test[degree]}")
    print(f"R2 Train Data: {R2_train[degree]}")
    print(f"R2 Test Data: {R2_test[degree]}")
    print("-" * 30)

# Plotting MSE values
plt.figure(figsize=(10, 6))
plt.plot(polydegree, MSE_train, label='MSE Train', marker='o', linestyle='-', color='blue',  markersize=5, linewidth=1)
plt.plot(polydegree, MSE_test, label='MSE Test', marker='o', linestyle='--', color='red',  markersize=5, linewidth=1)

#plot the MSE vs polydegree graph
plt.xlabel('Polynomial Degree', fontsize=14)
plt.ylabel('MSE', fontsize=14)
plt.title('MSE vs Polynomial Degree', fontsize=16)
plt.xticks(polydegree)  # Ensure all polynomial degrees are shown
plt.yticks(fontsize=12)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.legend(fontsize=12)
plt.tight_layout()  # Adjust layout to fit labels and title
plt.show()

degrees_to_plot = [2,3, 4, 5]

plt.figure(figsize=(10, 6))

# loop on the degrees to plot
for degree in degrees_to_plot:
    beta_values = betas[degree]  # Prendi i valori di beta per il grado corrente
    plt.plot(range(len(beta_values)), beta_values, marker='.', linestyle='-', label=f'Polynomial degree {degree}')

# add title and axes names etc
plt.xlabel('Index of Beta Coefficients', fontsize=14)
plt.ylabel('Beta Values', fontsize=14)
plt.title('Beta Values vs Polynomial Degree', fontsize=16)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.legend(fontsize=12)
plt.tight_layout()
plt.show()

"""# 10000 data"""

# 10000 data
np.random.seed(7935)

# Make data.
x = np.arange(0, 1, 0.01)
y = np.arange(0, 1, 0.01)
x, y = np.meshgrid(x,y)

z = FrankeFunction(x, y)
#add the noise to the franke function
f = FrankeFunction(x,y) + np.random.normal(0, 0.1, z.shape)

#flatten the franke function value into an array (it was a grid since we used mashgrid)
f = f.ravel()

#create vectors to store data
np.random.seed(7935)
maxdegree = 20
polydegree = np.zeros(maxdegree)
MSE_train = np.zeros(maxdegree)
MSE_test = np.zeros(maxdegree)
R2_train = np.zeros(maxdegree)
R2_test = np.zeros(maxdegree)

# for loop for every polynomial degree
for degree in range (maxdegree):

    # create design matrix
    X = design_matrix(x, y, degree)
    #divide into train and test data
    X_train, X_test, f_train, f_test = train_test_split(X, f, test_size=0.2)
    # calculate beta
    beta = calcuateB(X_train, f_train)
    #make the predictions
    ftilde = X_train @ beta
    fpred = X_test @ beta
    #fill the vectors with the data
    polydegree[degree] = degree
    MSE_train[degree] = calculateMSE(f_train, ftilde)
    MSE_test[degree] = calculateMSE(f_test, fpred)
    R2_train[degree] = R2(f_train, ftilde)
    R2_test[degree] = R2(f_test, fpred)

    #print the results
    print(f"Degree: {degree}")
    #print(f"Beta: {beta}")
    print(f"MSE Train Data: {MSE_train[degree]}")
    print(f"MSE Test Data: {MSE_test[degree]}")
    print(f"R2 Train Data: {R2_train[degree]}")
    print(f"R2 Test Data: {R2_test[degree]}")
    print("-" * 30)

# Plotting MSE values
plt.figure(figsize=(10, 6))
plt.plot(polydegree, MSE_train, label='MSE Train', marker='o', linestyle='-', color='blue',  markersize=5, linewidth=1)
plt.plot(polydegree, MSE_test, label='MSE Test', marker='o', linestyle='--', color='red',  markersize=5, linewidth=1)

#plot the MSE vs polydegree graph
plt.xlabel('Polynomial Degree', fontsize=14)
plt.ylabel('MSE', fontsize=14)
plt.title('MSE vs Polynomial Degree', fontsize=16)
plt.xticks(polydegree)  # Ensure all polynomial degrees are shown
plt.yticks(fontsize=12)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.legend(fontsize=12)
plt.tight_layout()  # Adjust layout to fit labels and title
plt.show()

"""# Ridge

"""

#RIDGE
np.random.seed(7935)

#choose the lambda values
lm_val = [1e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]

MSEridge_train = np.zeros((maxdegree,len(lm_val)))
MSEridge_test = np.zeros((maxdegree,len(lm_val)))
R2ridge_train = np.zeros((maxdegree,len(lm_val)))
R2ridge_test = np.zeros((maxdegree,len(lm_val)))

#start of the big loop
for i in range (maxdegree):
  X = design_matrix(x, y, i)
  X_train, X_test, f_train, f_test = train_test_split(X, f, test_size=0.2)
  #polynomial ridge regression
  for j, lmd in enumerate(lm_val):
      betaridge = betaRIDGE(X_train, f_train, lmd)
      ftildeRidge = X_train @ betaridge
      fpredictRidge = X_test @ betaridge
      #fill the matrices
      MSEridge_train[i, j] = calculateMSE(f_train, ftildeRidge)
      MSEridge_test[i, j] = calculateMSE(f_test, fpredictRidge)
      R2ridge_train[i, j] = R2(f_train, ftildeRidge)
      R2ridge_test[i, j] = R2(f_test, fpredictRidge)

  #plot the MSE vs lambda graph
  if i in range(maxdegree):
      plt.figure(figsize=(8, 6))
      plt.plot(lm_val, MSEridge_train[i], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.plot(lm_val, MSEridge_test[i], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.xscale('log')  # Scala logaritmica per lambda
      plt.xlabel('Lambda')
      plt.ylabel('MSE')
      plt.title(f'MSE vs Lambda for Polynomial Degree {i}')
      plt.legend()
      plt.grid(True)
      plt.show()

#plot the MSE vs polydegree graph
for k, lmd in enumerate(lm_val):
    #if k in [1, 3]:
        plt.figure(figsize=(8, 6))
        plt.plot(polydegree, MSEridge_train[:,k], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.plot(polydegree, MSEridge_test[:,k], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.xlabel('Polynomial Degree')
        plt.ylabel('MSE')
        plt.title(f'MSE vs Polynomial Degree for lambda {lm_val[k]}')
        plt.legend()
        plt.grid(True)
        plt.show()

plt.figure(figsize=(14, 6))

# Heatmap for test MSE Ridge
plt.subplot(1, 2, 1)
sns.heatmap(MSEridge_test, annot=True, fmt=".2f", cmap='viridis',
            xticklabels=lm_val, yticklabels=range(maxdegree),
            cbar_kws={'label': 'MSE'})
plt.title('Test MSE Heatmap (Ridge)', fontsize=16)
plt.xlabel('Lambda', fontsize=14)
plt.ylabel('Degree of Polynomial', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(False)

# Heatmap for train MSE Ridge
plt.subplot(1, 2, 2)
sns.heatmap(MSEridge_train, annot=True, fmt=".2f", cmap='viridis',
            xticklabels=lm_val, yticklabels=range(maxdegree),
            cbar_kws={'label': 'MSE'})
plt.title('Train MSE Heatmap (Ridge)', fontsize=16)
plt.xlabel('Lambda', fontsize=14)
plt.ylabel('Degree of Polynomial', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(False)

plt.tight_layout()
plt.show()

"""# Lasso

"""

#LASSO
np.random.seed(7935)
MSElasso_train = np.zeros((maxdegree,len(lm_val)))
MSElasso_test = np.zeros((maxdegree,len(lm_val)))
R2lasso_train = np.zeros((maxdegree,len(lm_val)))
R2lasso_test = np.zeros((maxdegree,len(lm_val)))

#start of the big loop
for i in range (maxdegree):
  X = design_matrix(x, y, i)
  X_train, X_test, f_train, f_test = train_test_split(X, f, test_size=0.2)

  #polynomial ridge regression
  for j, lmd in enumerate(lm_val):
      RegLasso = linear_model.Lasso(lmd,fit_intercept=True, max_iter=10000)
      RegLasso.fit(X_train,f_train)
      fpredictLasso = RegLasso.predict(X_test)
      ftildeLasso = RegLasso.predict(X_train)
      #fill the matrices
      MSElasso_train[i,j] = calculateMSE(f_train, ftildeLasso)
      MSElasso_test[i,j] = calculateMSE(f_test, fpredictLasso)
      R2lasso_train[i,j] = R2(f_train, ftildeLasso)
      R2lasso_test[i,j] = R2(f_test, fpredictLasso)

#plot the MSE vs lambda graph
  if i in range(maxdegree):
      plt.figure(figsize=(8, 6))
      plt.plot(lm_val, MSElasso_train[i], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.plot(lm_val, MSElasso_test[i], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.xscale('log')  # Scala logaritmica per lambda
      plt.xlabel('Lambda')
      plt.ylabel('MSE')
      plt.title(f'MSE vs Lambda for Polynomial Degree {i}')
      plt.legend()
      plt.grid(True)
      plt.show()

#plot the MSE vs polydegree graph
for k, lmd in enumerate(lm_val):
    #if k in [1, 3]:
        plt.figure(figsize=(8, 6))
        plt.plot(polydegree, MSElasso_train[:,k], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.plot(polydegree, MSElasso_test[:,k], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.xlabel('Polynomial Degree')
        plt.ylabel('MSE')
        plt.title(f'MSE vs Polynomial Degree for lambda {lm_val[k]}')
        plt.legend()
        plt.grid(True)
        plt.show()

# create the heatmaps
plt.figure(figsize=(14, 6))

# Heatmap for MSE Lasso
plt.subplot(1, 2, 1)
sns.heatmap(MSElasso_test, annot=True, fmt=".2f", cmap='viridis',
            xticklabels=lm_val, yticklabels=range(maxdegree),
            cbar_kws={'label': 'MSE'})
plt.title('Test MSE Heatmap (Lasso)', fontsize=16)
plt.xlabel('Lambda', fontsize=14)
plt.ylabel('Degree of Polynomial', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Heatmap for MSE Lasso
plt.subplot(1, 2, 2)
sns.heatmap(MSElasso_train, annot=True, fmt=".2f", cmap='viridis',
            xticklabels=lm_val, yticklabels=range(maxdegree),
            cbar_kws={'label': 'MSE'})
plt.title('Train MSE Heatmap (Lasso)', fontsize=16)
plt.xlabel('Lambda', fontsize=14)
plt.ylabel('Degree of Polynomial', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)


plt.tight_layout()
plt.show()

"""# Bias-variance trade-off (OLS)

"""

np.random.seed(7935)

#create vectors to store data
polydegree = np.zeros(maxdegree)
error = np.zeros(maxdegree)
bias = np.zeros(maxdegree)
variance = np.zeros(maxdegree)

n_boostraps = 200 #choose the number of bootstraps

for i in range(maxdegree):
    X = design_matrix(x, y, i)
    X_train, X_test, f_train, f_test = train_test_split(X, f, test_size=0.2)
    f_pred = np.empty((f_test.shape[0], n_boostraps))
    for j in range(n_boostraps):
        X_, f_ = resample(X_train, f_train)
        beta_resampled = calcuateB(X_, f_)
        f_pred[:, j] = X_test @ beta_resampled

    polydegree[i] = i
    #error[i] = np.mean( np.mean((f_test - f_pred)**2, axis=1, keepdims=True) )
    error[i] = np.mean(np.mean((f_test[:, np.newaxis] - f_pred)**2, axis=1, keepdims=True))
    bias[i] = np.mean( (f_test[:, np.newaxis] - np.mean(f_pred, axis=1, keepdims=True))**2 )
    variance[i] = np.mean( np.var(f_pred, axis=1, keepdims=True))
    print(f"Degree: {i}")
    print(f"MSE: {error[i]}")
    print(f"Bias: {bias[i]}")
    print(f"Variance: {variance[i]}")
    print('{} >= {} + {} = {}'.format(error[i], bias[i], variance[i], bias[i]+variance[i]))
    print("-" * 30)

plt.figure(figsize=(10, 6))
plt.plot(polydegree, error, label='Error', color='blue', marker='o', linestyle='--', markersize=5, linewidth=1)
plt.plot(polydegree, bias, label='Bias', color='red', marker='s', linestyle='--', markersize=5, linewidth=1)
plt.plot(polydegree, variance, label='Variance', color='green', marker='^', linestyle='--', markersize=5, linewidth=1)

plt.xlabel('Degree of Polynomial')
plt.ylabel('Value')
plt.title('Error, Bias, and Variance vs Degree of Polynomial')
plt.legend()
plt.grid(True)
plt.show()

"""# Cross- validation

## OLS
"""

deg = 8

np.random.seed(7935)
X = design_matrix(x, y, deg)

#create a k(number of folds) vector
K =[4,5,6,7, 8,9,10,11]

estimated_mse_KFold_test = []
estimated_mse_KFold_train = []

#loop on different values of k
for k in K:
  kfold = KFold(n_splits = k)
  scores_KFold_test = []
  scores_KFold_train = []
  for train_inds, test_inds in kfold.split(X):
    Xtrain = X[train_inds]
    ftrain = f[train_inds]
    Xtest = X[test_inds]
    ftest = f[test_inds]
    beta = calcuateB(Xtrain, ftrain)
    fpred = Xtest @ beta
    ftilde = Xtrain @ beta

    mse_test = calculateMSE(ftest, fpred)
    mse_train = calculateMSE(ftrain, ftilde)

    scores_KFold_test.append(calculateMSE(ftest, fpred))
    scores_KFold_train.append(calculateMSE(ftrain, ftilde))

    print(f"K={k}, Train MSE: {mse_train}, Test MSE: {mse_test}")

  estimated_mse_KFold_test.append(np.mean(scores_KFold_test)) # Append the mean to the list
  estimated_mse_KFold_train.append(np.mean(scores_KFold_train)) # Append the mean to the list

print (estimated_mse_KFold_test)
plt.figure()
plt.title(f'K-fold cross validation for degree {deg}')
plt.plot(K, estimated_mse_KFold_test, color='blue', marker='o', linestyle='--', markersize=5, linewidth=1, label = 'KFoldtest')
plt.plot(K, estimated_mse_KFold_train, color='red', marker='o', linestyle='--', markersize=5, linewidth=1, label = 'KFoldtrain')
plt.xlabel('K (number of folds)')
plt.ylabel('MSE')
#plt.ylim(-100, 1700)
plt.legend()
plt.show()

"""## Ridge"""

np.random.seed(7935)
deg = 13
X = design_matrix(x, y, deg)

lm_val = [1e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0] #lambda values
K = [4, 5, 6, 7, 8, 9, 10, 11]  # k values

#create vector to store datas
mse_test = np.zeros((len(K), len(lm_val)))
mse_train = np.zeros((len(K), len(lm_val)))

# K-Fold Cross-Validation
for k_index, k in enumerate(K):
    kfold = KFold(n_splits=k)

    for l_index, lmd in enumerate(lm_val):
        scores_KFold_test = []
        scores_KFold_train = []

        for train_inds, test_inds in kfold.split(X):
            Xtrain = X[train_inds]
            ftrain = f[train_inds]
            Xtest = X[test_inds]
            ftest = f[test_inds]
            beta = betaRIDGE(Xtrain, ftrain, lmd)
            fpred = Xtest @ beta
            ftilde = Xtrain @ beta
            scores_KFold_test.append(calculateMSE(ftest, fpred))
            scores_KFold_train.append(calculateMSE(ftrain, ftilde))
        mse_test[k_index, l_index] = np.mean(scores_KFold_test)
        mse_train[k_index, l_index] = np.mean(scores_KFold_train)

# Heatmap
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(mse_test, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Test MSE Heatmap Ridge')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.subplot(1, 2, 2)
sns.heatmap(mse_train, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Train MSE Heatmap Ridge')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.tight_layout()
plt.show()

"""## Lasso"""

np.random.seed(7935)
deg = 13
X = design_matrix(x, y, deg)

lm_val = [1e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0] #lambda values
K = [4, 5, 6, 7, 8, 9, 10, 11]  # k values

mse_test = np.zeros((len(K), len(lm_val)))
mse_train = np.zeros((len(K), len(lm_val)))

# K-Fold Cross-Validation
for k_index, k in enumerate(K):
    kfold = KFold(n_splits=k)

    for l_index, lmd in enumerate(lm_val):
        scores_KFold_test = []
        scores_KFold_train = []

        for train_inds, test_inds in kfold.split(X):
            Xtrain = X[train_inds]
            ftrain = f[train_inds]
            Xtest = X[test_inds]
            ftest = f[test_inds]

            RegLasso = linear_model.Lasso(lmd,fit_intercept=True, max_iter=10000)
            RegLasso.fit(Xtrain,ftrain)
            fpred = RegLasso.predict(Xtest)
            ftilde = RegLasso.predict(Xtrain)

            scores_KFold_test.append(calculateMSE(ftest, fpred))
            scores_KFold_train.append(calculateMSE(ftrain, ftilde))

        mse_test[k_index, l_index] = np.mean(scores_KFold_test)
        mse_train[k_index, l_index] = np.mean(scores_KFold_train)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(mse_test, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Test MSE Heatmap Lasso')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.subplot(1, 2, 2)
sns.heatmap(mse_train, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Train MSE Heatmap Lasso')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.tight_layout()
plt.show()

"""# Terrain data"""

terrain = imread('SRTM_data_Norway_1.tif')
np.random.seed(7935)
# Show the terrain in 2D
plt.figure()
plt.title('Terrain over Norway 1')
plt.imshow(terrain, cmap='gray')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()

# Size of the subset
N = 100

# Use a subset of the terrain data for analysis
terrain_subset = terrain[:N, :N]
z_shape = np.shape(terrain_subset)

# Create meshgrid for the subset
x = np.linspace(0, 1, z_shape[0])
y = np.linspace(0, 1, z_shape[1])
x, y = np.meshgrid(x, y)
z = terrain_subset

# Create a 3D plot
fig = plt.figure()
plt.figure(figsize=(20, 20))

ax = fig.add_subplot(111, projection='3d')

# Plot the surface
surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm, linewidth=0, antialiased=False)

# Customize the z axis
ax.set_zlim(np.min(z), np.max(z))
ax.zaxis.set_major_locator(LinearLocator(10))
ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))

# Add a color bar which maps values to colors
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()

#flatten the datas into arrays
x = x.flatten()
y = y.flatten()
f = z.flatten()

"""## OLS

"""

#create vectors to store data
maxdegree = 16
np.random.seed(7935)
polydegree = np.zeros(maxdegree)
MSE_train = np.zeros(maxdegree)
MSE_test = np.zeros(maxdegree)
R2_train = np.zeros(maxdegree)
R2_test = np.zeros(maxdegree)

# for loop for every polynomial degree
for degree in range (maxdegree):

    # create design matrix
    X = design_matrix(x, y, degree)

    #divide into train and test data and scale
    X_train, X_test, f_train, f_test = split_and_scale(X, f)

    #calculate beta
    beta = calcuateB(X_train, f_train)

    ftilde = X_train @ beta
    fpred = X_test @ beta
    polydegree[degree] = degree
    MSE_train[degree] = calculateMSE(f_train, ftilde)
    MSE_test[degree] = calculateMSE(f_test, fpred)
    R2_train[degree] = R2(f_train, ftilde)
    R2_test[degree] = R2(f_test, fpred)

    print(f"Degree: {degree}")
    #print(f"Beta: {beta}")
    print(f"MSE Train Data: {MSE_train[degree]}")
    print(f"MSE Test Data: {MSE_test[degree]}")
    print(f"R2 Train Data: {R2_train[degree]}")
    print(f"R2 Test Data: {R2_test[degree]}")
    print("-" * 30)


# Plotting MSE values
plt.figure(figsize=(10, 6))
plt.plot(polydegree, MSE_train, label='MSE Train', marker='o', linestyle='-', color='blue',  markersize=5, linewidth=1)
plt.plot(polydegree, MSE_test, label='MSE Test', marker='o', linestyle='--', color='red',  markersize=5, linewidth=1)

plt.xlabel('Polynomial Degree', fontsize=14)
plt.ylabel('MSE', fontsize=14)
plt.title('MSE vs Polynomial Degree', fontsize=16)
plt.xticks(polydegree)  # Ensure all polynomial degrees are shown
plt.yticks(fontsize=12)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.legend(fontsize=12)
plt.tight_layout()  # Adjust layout to fit labels and title
plt.show()

"""## Ridge"""

lm_val = [1e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]
np.random.seed(7935)
MSEridge_train = np.zeros((maxdegree,len(lm_val)))
MSEridge_test = np.zeros((maxdegree,len(lm_val)))
R2ridge_train = np.zeros((maxdegree,len(lm_val)))
R2ridge_test = np.zeros((maxdegree,len(lm_val)))

#start of the big loop
for i in range (maxdegree):
  X = design_matrix(x, y, i)
  X_train, X_test, f_train, f_test = split_and_scale(X, f)
  #polynomial ridge regression (both with my model and scikt)
  for j, lmd in enumerate(lm_val):
      betaridge = betaRIDGE(X_train, f_train, lmd)
      ftildeRidge = X_train @ betaridge
      fpredictRidge = X_test @ betaridge
      #fill the matrices
      MSEridge_train[i, j] = calculateMSE(f_train, ftildeRidge)
      MSEridge_test[i, j] = calculateMSE(f_test, fpredictRidge)
      R2ridge_train[i, j] = R2(f_train, ftildeRidge)
      R2ridge_test[i, j] = R2(f_test, fpredictRidge)

  if i in range(maxdegree):
      plt.figure(figsize=(8, 6))
      plt.plot(lm_val, MSEridge_train[i], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.plot(lm_val, MSEridge_test[i], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.xscale('log')
      plt.xlabel('Lambda')
      plt.ylabel('MSE')
      plt.title(f'MSE vs Lambda for Polynomial Degree {i}')
      plt.legend()
      plt.grid(True)
      plt.show()

for k, lmd in enumerate(lm_val):
    #if k in [1, 3]:
        plt.figure(figsize=(8, 6))
        plt.plot(polydegree, MSEridge_train[:,k], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.plot(polydegree, MSEridge_test[:,k], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.xlabel('Polynomial Degree')
        plt.ylabel('MSE')
        plt.title(f'MSE vs Polynomial Degree for lambda {lm_val[k]}')
        plt.legend()
        plt.grid(True)
        plt.show()

#create the heatmap for ridge regression
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(MSEridge_test, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=range(maxdegree) )
ax.set_aspect('equal', adjustable='box')
plt.title('Test MSE Heatmap Ridge')
plt.xlabel('Lambda')
plt.ylabel('Degree of Polynomial')

plt.subplot(1, 2, 2)
sns.heatmap(MSEridge_train, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=range(maxdegree))
plt.title('Train MSE Heatmap Ridge')
plt.xlabel('Lambda')
plt.ylabel('Degree of Polynomial')

plt.tight_layout()
plt.show()

"""## Lasso"""

MSElasso_train = np.zeros((maxdegree,len(lm_val)))
MSElasso_test = np.zeros((maxdegree,len(lm_val)))
R2lasso_train = np.zeros((maxdegree,len(lm_val)))
R2lasso_test = np.zeros((maxdegree,len(lm_val)))

np.random.seed(7935)
#start of the big loop
for i in range (maxdegree):
  X = design_matrix(x, y, i)
  X_train, X_test, f_train, f_test = split_and_scale(X, f)

  #polynomial ridge regression (both with my model and scikt)
  for j, lmd in enumerate(lm_val):
      RegLasso = linear_model.Lasso(lmd,fit_intercept=True, max_iter=10000)
      RegLasso.fit(X_train,f_train)
      fpredictLasso = RegLasso.predict(X_test)
      ftildeLasso = RegLasso.predict(X_train)
      #fill the matrices
      MSElasso_train[i,j] = calculateMSE(f_train, ftildeLasso)
      MSElasso_test[i,j] = calculateMSE(f_test, fpredictLasso)
      R2lasso_train[i,j] = R2(f_train, ftildeLasso)
      R2lasso_test[i,j] = R2(f_test, fpredictLasso)

  if i in range(maxdegree):
      plt.figure(figsize=(8, 6))
      plt.plot(lm_val, MSElasso_train[i], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.plot(lm_val, MSElasso_test[i], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
      plt.xscale('log')  # Scala logaritmica per lambda
      plt.xlabel('Lambda')
      plt.ylabel('MSE')
      plt.title(f'MSE vs Lambda for Polynomial Degree {i}')
      plt.legend()
      plt.grid(True)
      plt.show()

for k, lmd in enumerate(lm_val):
    #if k in [1, 3]:
        plt.figure(figsize=(8, 6))
        plt.plot(polydegree, MSElasso_train[:,k], label='MSE Train', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.plot(polydegree, MSElasso_test[:,k], label='MSE Test', marker='o', linestyle='--', markersize=5, linewidth=1)
        plt.xlabel('Polynomial Degree')
        plt.ylabel('MSE')
        plt.title(f'MSE vs Polynomial Degree for lambda {lm_val[k]}')
        plt.legend()
        plt.grid(True)
        plt.show()

#create the heatmap for lasso regression
plt.figure(figsize=(12, 6))
np.random.seed(7935)
plt.subplot(1, 2, 1)
sns.heatmap(MSElasso_test, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=range(maxdegree) )
ax.set_aspect('equal', adjustable='box')
plt.title('Test MSE Heatmap Lasso')
plt.xlabel('Lambda')
plt.ylabel('Degree of Polynomial')

plt.subplot(1, 2, 2)
sns.heatmap(MSElasso_train, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=range(maxdegree))
plt.title('Train MSE Heatmap Lasso')
plt.xlabel('Lambda')
plt.ylabel('Degree of Polynomial')

plt.tight_layout()
plt.show()

"""## Bias-variance trade-off"""

maxdegree= 16 #choose the maximum degree
polydegree = np.zeros(maxdegree)
error = np.zeros(maxdegree)
bias = np.zeros(maxdegree)
variance = np.zeros(maxdegree)

n_boostraps = 200 #choose number of bootsraps

for i in range(maxdegree):
    X = design_matrix(x, y, i)
    X_train, X_test, f_train, f_test = split_and_scale(X, f)
    f_pred = np.empty((f_test.shape[0], n_boostraps))
    for j in range(n_boostraps):
        X_, f_ = resample(X_train, f_train)
        beta_resampled = calcuateB(X_, f_)
        f_pred[:, j] = X_test @ beta_resampled

    polydegree[i] = i
    #error[i] = np.mean( np.mean((f_test - f_pred)**2, axis=1, keepdims=True) )
    error[i] = np.mean(np.mean((f_test[:, np.newaxis] - f_pred)**2, axis=1, keepdims=True))
    bias[i] = np.mean( (f_test[:, np.newaxis] - np.mean(f_pred, axis=1, keepdims=True))**2 )
    variance[i] = np.mean( np.var(f_pred, axis=1, keepdims=True))
    #print(f"Degree: {i}")
    #print(f"MSE: {error[i]}")
    #print(f"Bias: {bias[i]}")
    #print(f"Variance: {variance[i]}")
    #print('{} >= {} + {} = {}'.format(error[i], bias[i], variance[i], bias[i]+variance[i]))
    #print("-" * 30)

plt.figure(figsize=(10, 6))
plt.plot(polydegree, error, label='Error', color='blue', marker='o')
plt.plot(polydegree, bias, label='Bias', color='red', marker='s')
plt.plot(polydegree, variance, label='Variance', color='green', marker='^')
plt.xlabel('Degree of Polynomial')
plt.ylabel('Value')
plt.title('Error, Bias, and Variance vs Degree of Polynomial')
plt.legend()
plt.grid(True)
plt.show()

"""## Cross validation

### OLS
"""

deg = 9 #choose the degree for the cross validation

X = design_matrix(x, y, deg)
np.random.seed(7935)

K =[4,5,6,7, 8,9,10,11]
estimated_mse_KFold_test = []
estimated_mse_KFold_train = []

#loop on different values of K
for k in K:
  kfold = KFold(n_splits = k)
  scores_KFold_test = []
  scores_KFold_train = []
  for train_inds, test_inds in kfold.split(X):
    Xtrain = X[train_inds]
    ftrain = f[train_inds]
    Xtest = X[test_inds]
    ftest = f[test_inds]

    scaler_X = StandardScaler(with_std=True)
    scaler_X.fit(Xtrain)
    scaler_f = StandardScaler(with_std=True)
    scaler_f.fit(ftrain.reshape(-1, 1))

    #scale the datas (in this case we could not use the function defined by us)
    Xtrain = scaler_X.transform(Xtrain)
    Xtest = scaler_X.transform(Xtest)
    ftrain = scaler_f.transform(ftrain.reshape(-1, 1)).ravel()
    ftest = scaler_f.transform(ftest.reshape(-1, 1)).ravel()

    beta = calcuateB(Xtrain, ftrain)
    fpred = Xtest @ beta
    ftilde = Xtrain @ beta

    scores_KFold_test.append(calculateMSE(ftest, fpred))
    scores_KFold_train.append(calculateMSE(ftrain, ftilde))

  estimated_mse_KFold_test.append(np.mean(scores_KFold_test)) # Append the mean to the list
  estimated_mse_KFold_train.append(np.mean(scores_KFold_train)) # Append the mean to the list

plt.figure()
plt.title(f'K-fold cross validation for degree {deg}')
plt.plot(K, estimated_mse_KFold_test, color='blue', marker='o', linestyle='--', markersize=5, linewidth=1, label = 'KFoldtest')
plt.plot(K, estimated_mse_KFold_train, color='red', marker='o', linestyle='--', markersize=5, linewidth=1, label = 'KFoldtrain')
plt.xlabel('K (number of folds)')
plt.ylabel('MSE')
plt.legend()
plt.show()

"""### Ridge"""

deg = 9 #choose degree for cross validation
X = design_matrix(x, y, deg)

np.random.seed(7935)
lm_val = [1e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]
K = [4, 5, 6, 7, 8, 9, 10, 11]  # Valori di k

mse_test = np.zeros((len(K), len(lm_val)))
mse_train = np.zeros((len(K), len(lm_val)))

# K-Fold Cross-Validation
for k_index, k in enumerate(K):
    kfold = KFold(n_splits=k)

    for l_index, lmd in enumerate(lm_val):
        scores_KFold_test = []
        scores_KFold_train = []

        for train_inds, test_inds in kfold.split(X):
            Xtrain = X[train_inds]
            ftrain = f[train_inds]
            Xtest = X[test_inds]
            ftest = f[test_inds]
            scaler_X = StandardScaler(with_std=True)
            scaler_X.fit(Xtrain)
            scaler_f = StandardScaler(with_std=True)
            scaler_f.fit(ftrain.reshape(-1, 1))

            #scale the datas (in this case we could not use the function defined by us)
            Xtrain = scaler_X.transform(Xtrain)
            Xtest = scaler_X.transform(Xtest)
            ftrain = scaler_f.transform(ftrain.reshape(-1, 1)).ravel()
            ftest = scaler_f.transform(ftest.reshape(-1, 1)).ravel()

            beta = betaRIDGE(Xtrain, ftrain, lmd)

            fpred = Xtest @ beta
            ftilde = Xtrain @ beta

            scores_KFold_test.append(calculateMSE(ftest, fpred))
            scores_KFold_train.append(calculateMSE(ftrain, ftilde))

        mse_test[k_index, l_index] = np.mean(scores_KFold_test)
        mse_train[k_index, l_index] = np.mean(scores_KFold_train)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(mse_test, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Test MSE Heatmap Ridge')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.subplot(1, 2, 2)
sns.heatmap(mse_train, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Train MSE Heatmap Ridge')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.tight_layout()
plt.show()

"""### Lasso"""

#lasso cross validation

deg = 9 #choose the polynomial degree
X = design_matrix(x, y, deg)
np.random.seed(7935)
lm_val = [1e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]
K = [4, 5, 6, 7, 8, 9, 10, 11]  # Valori di k

mse_test = np.zeros((len(K), len(lm_val)))
mse_train = np.zeros((len(K), len(lm_val)))

# K-Fold Cross-Validation
for k_index, k in enumerate(K):
    kfold = KFold(n_splits=k)

    for l_index, lmd in enumerate(lm_val):
        scores_KFold_test = []
        scores_KFold_train = []

        for train_inds, test_inds in kfold.split(X):
            Xtrain = X[train_inds]
            ftrain = f[train_inds]
            Xtest = X[test_inds]
            ftest = f[test_inds]
            scaler_X = StandardScaler(with_std=True)
            scaler_X.fit(Xtrain)
            scaler_f = StandardScaler(with_std=True)
            scaler_f.fit(ftrain.reshape(-1, 1))

            #scale the datas (in this case we could not use the function defined by us)
            Xtrain = scaler_X.transform(Xtrain)
            Xtest = scaler_X.transform(Xtest)
            ftrain = scaler_f.transform(ftrain.reshape(-1, 1)).ravel()
            ftest = scaler_f.transform(ftest.reshape(-1, 1)).ravel()

            RegLasso = linear_model.Lasso(lmd,fit_intercept=True, max_iter=10000) #we reduced the number of iteration because of running time
            RegLasso.fit(Xtrain,ftrain)
            fpred = RegLasso.predict(Xtest)
            ftilde = RegLasso.predict(Xtrain)

            scores_KFold_test.append(calculateMSE(ftest, fpred))
            scores_KFold_train.append(calculateMSE(ftrain, ftilde))

        mse_test[k_index, l_index] = np.mean(scores_KFold_test)
        mse_train[k_index, l_index] = np.mean(scores_KFold_train)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.heatmap(mse_test, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Test MSE Heatmap Lasso')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.subplot(1, 2, 2)
sns.heatmap(mse_train, annot=True, fmt=".2f", cmap='viridis', xticklabels=lm_val, yticklabels=K)
plt.title('Train MSE Heatmap Lasso')
plt.xlabel('Lambda')
plt.ylabel('K (Number of Folds)')

plt.tight_layout()
plt.show()